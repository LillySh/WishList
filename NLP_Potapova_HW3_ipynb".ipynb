{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LillySh/WishList/blob/main/NLP_Potapova_HW3_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метод similarity\n",
        "\n",
        "У всех типов объектов-контейнеров в библиотеке spaCy есть метод similarity, который используется для вычисления оценки семантического подобия двух объектов-контейнеров произвольного типа посредством сравнения их векторов слов. Для вычисления подобия интервалов и документов, не обладающих векторами слов, spaCy сравнивает средние значения векторов слов содержащихся в них\n",
        "токенов."
      ],
      "metadata": {
        "id": "BhLv2Qoz1h67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычислим семантическое подобие двух объектов-контейнеров для различных объектов. В коде ниже вычислим оценку семантического подобия между предложением I want a green apple. и вырезанной из него фразой a green apple.\n",
        "\n",
        "Как видим, вычисленная оценка подобия достаточно высока, чтобы считать содержимое обоих объектов схожим (диапазон значений степени подобия от 0 до 1)."
      ],
      "metadata": {
        "id": "nl2KbUG5TcK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ54ie6i-CKX",
        "outputId": "8b77d6e8-d55e-491d-b291-c950a3c7aa8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "<ipython-input-1-94f975ecb8dd>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  doc.similarity(doc[2:5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6240444557468489"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc=nlp('I want a green apple.')\n",
        "doc.similarity(doc[2:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравнение объекта самого с собой. Логично, что вернется значение 1."
      ],
      "metadata": {
        "id": "NthvoD7cTcov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc.similarity(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ3OBlP--cRn",
        "outputId": "1a37e301-4e19-4a5f-fa53-d9f35d57ff98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2:5].similarity(doc[2:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBrEUNVJ-dAh",
        "outputId": "a15e4d33-df2d-4417-c98e-b7f3bd065684"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее сравним хранящееся в doc2 предложение I like red oranges и интервал a green apple, извлеченный из doc. \n",
        "\n",
        "В данном случае степень подобия оказалась мала.  Так как указаны разные фрукты в предложении (метод similarity это учитывает) и глаголы want и like выражают разные состояния."
      ],
      "metadata": {
        "id": "CizScJjYTdNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2=nlp('I like red oranges.')\n",
        "doc2.similarity(doc[2:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFRZ71g--dDP",
        "outputId": "2f8101e7-d337-433b-96f8-70156f01dc1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-29dc56b362d7>:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  doc2.similarity(doc[2:5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22478505229070148"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее сравним объект Token oranges с объектом Span, содержащим один токен apple."
      ],
      "metadata": {
        "id": "ud49L0fhTdqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = doc2[3:4][0]\n",
        "token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XK1hbFN-dF6",
        "outputId": "bab2c017-cbfe-4a55-c1fd-e445c887611f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "oranges"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.similarity(doc[4:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzI3-wmCyjlO",
        "outputId": "f51e3ef9-f79a-4fae-f4d0-5bf1e06a4fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2c624528aea6>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  token.similarity(doc[4:5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5848100781440735"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде всего, с помощью ссылки на первый элемент интервала преобразовали явным образом объект Span с одним токеном oranges в объект Token, а затем вычислили степень его подобия интервалу apple."
      ],
      "metadata": {
        "id": "5LXULuOZTeMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод similarity() распознает слова, которые относятся к одной или\n",
        "нескольким близким категориям и часто встречаются во взаимосвязанных контекстах, а также возвращает для подобных слов высокие\n",
        "значения уровня подобия."
      ],
      "metadata": {
        "id": "6RrBk1oITewE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Применение семантического подобия для задач категоризации\n",
        "\n",
        "Вычисление семантического подобия двух объектов позволяет распределять тексты по категориям, а также выбирать из корпуса лишь те из них, которые относятся к нужной теме. \n",
        "\n",
        "Далее из всех комментариев пользователей на сайте нам нужно выбрать комментарии, связанные со словом fruits. В нашей задаче мы будем анализировать следующие комментарии:\n",
        "* I want to buy this beautiful book at the end of the week.\n",
        "* Sales of citrus have increased over the last year.\n",
        "* How much do you know about this type of tree?\n",
        "\n",
        "\n",
        "Чтобы выбрать это с помощью кода, необходимо сравним каждое предложение со словом fruits. \n",
        "\n",
        "Далее создадим объект Token для слова fruits и применим конвейер к категоризируемым предложениям, создавая для них общий объект Doc. Далее разрезаем его на предложения, а затем выводим в консоль каждое из них и его семантическое подобие к токену fruits, которое вычисляется с помощью метода similarity объекта этого токена."
      ],
      "metadata": {
        "id": "6WN7AanKxHh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "token = nlp(u'fruits')[0]\n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?')\n",
        "for sent in doc.sents: #разделение на предложения\n",
        "  print(sent.text)\n",
        "  print('similarity to', token.text, 'is', token.similarity(sent),'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICd18k1p-cVz",
        "outputId": "b3c093de-d800-480d-fc30-581a5deb04ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to buy this beautiful book at the end of the week.\n",
            "similarity to fruits is -0.04223029315471649 \n",
            "\n",
            "Sales of citrus have increased over the last year.\n",
            "similarity to fruits is -0.027368908748030663 \n",
            "\n",
            "How much do you know about this type of tree?\n",
            "similarity to fruits is -0.07132802903652191 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-52ad01cea877>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print('similarity to', token.text, 'is', token.similarity(sent),'\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что степень подобия слова fruits к каждому предложению крайне мала. Это еще зависит от используемой модели."
      ],
      "metadata": {
        "id": "yib9_IUW1HLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выделение существительных как шаг предварительной обработки\n",
        "\n",
        "Более удачный подход к категоризации -  выделять только самые значимые слова и сравнивать только их. Подобная подготовка текста к обработке, которая называется предварительной обработкой, позволяет существенно повысить успешность опе-\n",
        "раций NLP.\n",
        "\n",
        "То есть теперь вместо сравнения векторов слов для целого объекта будем сравнивать векторы слов только для определенных частей речи."
      ],
      "metadata": {
        "id": "WV2dbAbD1ats"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнем с описания токена fruits, который будем использовать для ряда\n",
        "сравнений. Проходя в цикле по токенам каждого предложения, выделяем существительные и сохраняем их в списке Python. Затем склеиваем существительные из этого списка в простую строку,\n",
        "которую позже преобразуем в объект Doc. Далее сравниваем объект Doc с токеном fruits, чтобы определить степень их семантического подобия. Значение семантического подобия сохраняется в ассоциативном\n",
        "массиве Python, который затем выводится в консоль"
      ],
      "metadata": {
        "id": "jmQ1uyz2Tfwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "token = nlp(u'fruits')[0] #описания токена\n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?')\n",
        "similarity = {}\n",
        "for i, sent in enumerate(doc.sents):#проходим по каждому предложению\n",
        "  noun_span_list = [sent[j].text for j in range(len(sent)) if #выделяем существительные и записываем их\n",
        "  sent[j].pos_ == 'NOUN']\n",
        "  noun_span_str = ' '.join(noun_span_list) #склеиваем существительные в строку\n",
        "  noun_span_doc = nlp(noun_span_str) #преобразуем строку с существительными в объект doc\n",
        "  similarity.update({i:token.similarity(noun_span_doc)}) #подобие сохраняем в ассоциативный массив\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN_UWL3KPQ0k",
        "outputId": "4fa09540-7dc7-4ea7-cac4-429cddc67a01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.0726100970506393, 1: 0.2306078439472607, 2: 0.31927317721232235}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-8b19815d5d94>:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity.update({i:token.similarity(noun_span_doc)}) #подобие сохраняем в ассоциативный массив\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим степень семантического подобия со словом fruits выше для всех предложений."
      ],
      "metadata": {
        "id": "8Ky1oQaiykpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выделение и сравнение именованных сущностей\n",
        "\n",
        "\n",
        "В некоторых случаях вместо выделения всех существительных из сравниваемых текстов имеет смысл выделять лишь определенный вид,например именованные сущности.\n",
        "\n",
        "Далее в коде мы группируем объекты Doc с примерами текстов в список, чтобы по ним можно было пройти в цикле. Объявляем ассоциативный массив Python для хранения ключевых слов всех текстов. Проходим в цикле по объектам Doc, выделяя ключевые слова в отдельные списки для каждого из текстов. При этом выбор падает лишь на слова, маркированные как именованные сущности. Далее выводим список в консоль, чтобы посмотреть его содержимое. Трансформируем этот список в простую строку, к которой применяем конвейер, а затем преобразуем ее в объект Doc. Наконец, добавляем объект Doc к объявленному ранее ассоциативному массиву spans."
      ],
      "metadata": {
        "id": "3xYqZg8JTglt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc1 = nlp(u'Google Search, often referred to as simply Google, is the most used search engine nowadays. It handles a huge number of searches each day.')\n",
        "\n",
        "doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.')\n",
        "\n",
        "doc3 = nlp(u\"Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.\")\n",
        "docs = [doc1,doc2,doc3] #группировка объектов\n",
        "spans = {}#объявление ассоциативного массива для хранения ключевых слов\n",
        "for j,doc in enumerate(docs):#проход в цикле по объектам, выделяя ключевые слова\n",
        "  named_entity_span = [doc[i].text for i in range(len(doc)) if#ключевые являются существительные\n",
        "  doc[i].ent_type != 0]\n",
        "  print(named_entity_span)#вывод в консоли\n",
        "  named_entity_span = ' '.join(named_entity_span)#трансформация в простую строку, которая преобразуется в конвейер\n",
        "  named_entity_span = nlp(named_entity_span)#преобразование в объект doc\n",
        "  spans.update({j:named_entity_span})#добавление объект doc к массиву"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKD06MIAPffv",
        "outputId": "216f7386-2af9-4294-8e31-848dd5a512ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Google', 'Search', 'Google', 'each', 'day']\n",
            "['Microsoft', 'Windows', 'Microsoft']\n",
            "['Titicaca', 'Andes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, мы получили набор именованных сущностей из каждогт предложения."
      ],
      "metadata": {
        "id": "adDFLr5t3pI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вызываем метод similarity() для каждого из интервалов и выводим результаты\n"
      ],
      "metadata": {
        "id": "svWECcn9QNbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
        "print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
        "print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22AWtaV0Pfiu",
        "outputId": "8b8277e1-0304-437f-84ca-97da4b39e05c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1 is similar to doc2: 0.7054618244550298\n",
            "doc1 is similar to doc3: 0.6104506459201123\n",
            "doc2 is similar to doc3: 0.6308411086910213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-3ba975a8ccfc>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
            "<ipython-input-11-3ba975a8ccfc>:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
            "<ipython-input-11-3ba975a8ccfc>:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, полученные значения указывают, что наиболее подобны первый и второй тексты, посвященные американским IT-компаниям."
      ],
      "metadata": {
        "id": "fAzNZPxS32uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Поиск паттернов лингвистических признаков\n",
        "\n",
        "Поиск паттернов в текстах необходимо по той причине, что в большинстве случаев в тексте не бывает двух совершенно одинаковых предложений, так как обычно текст состоит из разных предложений, содержащих разные слова. Так как писать отдельный код для обработки каждого предложения текста нерационально.\n",
        "\n",
        "Поскольку число слов в анализируемых предложениях одинаково, можно пройти по словам обоих предложений в одном цикле. Если метка зависимости одинакова для слов с идентичными индексами, то выводим эти слова, присвоенную им метку и ее описание."
      ],
      "metadata": {
        "id": "CAYMsuw82B3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc1 = nlp(u'We can overtake them.')\n",
        "doc2 = nlp(u'You must specify it.')\n",
        "for i in range(len(doc1)-1):\n",
        "  if doc1[i].dep_ == doc2[i].dep_:\n",
        "    print(doc1[i].text, doc2[i].text, doc1[i].dep_,\n",
        "  spacy.explain(doc1[i].dep_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k61yLqnQPfls",
        "outputId": "611ef6af-6672-4129-95b2-1a44aef6f33c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We You nsubj nominal subject\n",
            "can must aux auxiliary\n",
            "overtake specify ROOT root\n",
            "them it dobj direct object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что список меток зависимостей для обоих предложений. Это означает, что предложение строятся по одному паттерну последовательности слов, который описывается метками синтакси-\n",
        "ческой зависимости таким образом: «подлежащее + вспомогательный глагол + глагол + прямое дополнение»."
      ],
      "metadata": {
        "id": "ly0B1cOe5B5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проверка высказывания на соотвествие паттерну.\n",
        "\n",
        "Опишем функцию, реализующую указанный паттерн, а затем протестируем ее на примере предложения:"
      ],
      "metadata": {
        "id": "DTq9JUfMThj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже описана функция dep_pattern, принимающая в качестве- параметра объект Doc. В ней проходим в цикле по токенам объекта Doc и ищем паттерн «подлежащее + вспомогательный глагол + глагол».\n",
        "\n",
        "\n",
        "Найдя его, проверяем, есть ли среди непосредственных дочерних элементов глагола прямое дополнение. В случае обнаружения этого прямого дополнения функция вернет True, в противном случае — False"
      ],
      "metadata": {
        "id": "-zyqpMUPTiDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def dep_pattern(doc):#принимаем параметр объекта doc\n",
        "  for i in range(len(doc)-1):#проход в цикле\n",
        "    if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and doc[i+2].dep_ == 'ROOT':#ищем паттерн подлежащее+вспом глагол+глагол\n",
        "      for tok in doc[i+2].children:#проверка на наличие прямого дополнения\n",
        "        if tok.dep_ == 'dobj':\n",
        "          return True#нашли прямое дополнение\n",
        "  return False#\n",
        "doc = nlp(u'We can overtake them.')#передаем объект doc\n",
        "if dep_pattern(doc):#\n",
        "  print('Found')\n",
        "else:\n",
        "  print('Not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDi_7o0KPfrF",
        "outputId": "247f6b73-a6c8-4097-93f3-23b5af9ec2f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как образец предложения из данного примера соответствует\n",
        "указанному паттерну, сценарий выдаст в консоль:\n",
        "\n",
        "`Found`\n",
        "\n",
        "## Использование утилиты Matcher библиотеки spaCy для поиска паттернов последовательностей слов.\n",
        "\n",
        "\n",
        "В библиотеке spaCy есть встроенная утилита для решения подобной задачи — Matcher,\n",
        "специально разработанная для поиска последовательностей токенов, соответствующих заданному паттерну.\n",
        "\n",
        "Ниже реализуем паттерн «подлежащее + вспомогательный глагол + глагол» с помощью Matcher \n",
        "\n",
        "Создаем экземпляр Matcher, передавая конструктору объект со словарным запасом, соответствующим документам, которые этот Matcher будет обрабатывать. Далее описываем паттерн, указывая метки зависимости, которым должна удовлетворять последовательность слов.\n",
        "Добавляем только что созданный паттерн в Matcher.\n",
        "Применяем Matcher к нашему примеру текста и получаем список соответствующих паттерну токенов, а затем проходим по этому списку\n",
        "в цикле, выводя начальную и конечную позиции токенов паттерна в тексте"
      ],
      "metadata": {
        "id": "tNDEQVIRTio4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)#создаем экземпляр\n",
        "pattern = [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\": \"ROOT\"}]#описание паттерна\n",
        "matcher.add(\"NsubjAuxRoot\", [pattern])#применяем matcher к тексту и получаем список соответсвующих токенов\n",
        "doc = nlp(u\"We can overtake them.\")\n",
        "matches = matcher(doc)#\n",
        "for match_id, start, end in matches:#проход в цикле по списку\n",
        "  span = doc[start:end]\n",
        "  print(\"Span: \", span.text)#выводим начальную и конечную позицию токенов\n",
        "  print(\"The positions in the doc are: \", start, \"-\", end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ti0BJUlPfuX",
        "outputId": "831aab9d-c17a-4491-efa6-9489260fbc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span:  We can overtake\n",
            "The positions in the doc are:  0 - 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В результате получаем начальную и конечную позицию слов, составляющих последовательность, которая\n",
        "соответствует заданному паттерну.\n",
        "\n",
        "Такой подход очень удобен для последовательностей слов, которые расположены одно за другим."
      ],
      "metadata": {
        "id": "M4NdsKPB8JAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Применение нескольких паттернов\n",
        "\n",
        "Помимо последовательности меток зависимостей, описанной в подразделе «Проверка высказывания на соответствие паттерну» можно описать новую функцию, реализующую паттерн, основанный на\n",
        "тегах частей речи. Например, описываемый тегами частей речи паттерн будет содержать условие, что подлежащее и прямое дополнение должны быть личными местоимениями. Эта новая функция может среди про-\n",
        "чего реализовывать паттерн «личное местоимение + вспомогательный модальный глагол + глагол в инфинитиве + ... + личное местоимение...»."
      ],
      "metadata": {
        "id": "Hh7_TuZR7UsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция описанная ниже в случае обнаружении несоответствия возвращает False. В противном случае по завершении всех проверок и при отсутствии каких-либо несоответствий функция возвращает True.\n",
        "\n",
        "Для проверки на паттерны применяем к предложению конвейер, после чего проверяем, удовлетворяет ли оно обоим паттернам. Поскольку используемый в этом примере образец предложения удовлетворяет обоим паттернам, должно появиться следующее:\n",
        "\n",
        "`Found`\n",
        "\n",
        "Если же заменить образец предложения на такой: I might send them a card as a reminder., будет выведено:\n",
        "\n",
        "`Not found`\n",
        "\n"
      ],
      "metadata": {
        "id": "pr-0EafTTjPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def pos_pattern (doc):#описываем функцию\n",
        "  for token in doc:\n",
        "    #Проверка на соответствие части предложения определенному тегу\n",
        "    if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
        "      return False\n",
        "    if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
        "      return False\n",
        "    if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
        "      return False\n",
        "    if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
        "      return False#при несоответствии \n",
        "  return True#соответствие\n",
        "#Тестирование кода\n",
        "doc = nlp(u'We can overtake them.')\n",
        "if dep_pattern(doc) and pos_pattern(doc):#проверка удовлетворению\n",
        "  print('Found')\n",
        "else:\n",
        "  print('Not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BllWmnhTeiM",
        "outputId": "f5d7017d-6d1c-41b4-827a-fd5d50f8aa84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, полученный результат означает, что используемый в этом примере образец предложения удовлетворяет обоим паттернам."
      ],
      "metadata": {
        "id": "bHs3EHXM99vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание паттернов на основе пользовательских признаков\n",
        "\n",
        "При создании паттерна последовательности слов может возникнуть необходимость расширить функциональность лингвистических признаков, предоставляемых spaCy, подогнав их под свои задачи. Например, может понадобиться, чтобы предыдущий сценарий распознавал паттерн, в котором учитывается число местоимений (единственное или множественное). Это полезно в том случае, если нужно найти существительное, к которому относится местоимение. \n",
        "\n",
        "Однако SpaCy не различает число местоимений."
      ],
      "metadata": {
        "id": "iA8GC5uJ8a9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В функции pron_\n",
        "pattern описываем список Python, включающий все возможные личные местоимения во множественном числе. Далее описываем цикл,\n",
        "в котором проходим по токенам полученного предложения в поисках\n",
        "прямого дополнения — личного местоимения. Найдя, проверяем,\n",
        "встречается ли оно в списке личных местоимений во множественном\n",
        "числе. Если да, наша функция возвращает plural. В противном\n",
        "случае она возвращает singular. Если функции не удалось обнаружить прямое дополнение или оно не является личным местоимением,\n",
        "возвращается Not found."
      ],
      "metadata": {
        "id": "fu0J3fGETj17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pron_pattern(doc):\n",
        "  plural = ['we','us','they','them']#описываем список, включающий личные местоимения\n",
        "  for token in doc:\n",
        "    if token.dep_ == 'dobj' and token.tag_ == 'PRP':#поиск прямого дополнения - личного местоимения\n",
        "      if token.text in plural:\n",
        "        return 'plural'#при нахождении\n",
        "      else:\n",
        "        return 'singular'#при отсутствии\n",
        "  return 'not found'#если нет прямого дополнения или оно не является личным местоимением\n",
        "doc = nlp(u'We can overtake them.')\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "  print('Found:', 'the pronoun in position of direct object is',\n",
        "  pron_pattern(doc))\n",
        "else:\n",
        "  print('Not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubdOfmM9WPco",
        "outputId": "408cfadd-f168-49ab-9d82-87a0782642a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: the pronoun in position of direct object is plural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, функция вернула `plural`, что означает, что в предложении встретилось личное местоимение во множественном числе."
      ],
      "metadata": {
        "id": "jnuaVu0fNHt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Применение паттернов последовательностей слов в чат-бот для генерации высказываний\n",
        "\n",
        "Как упоминалось ранее, самые трудные задачи NLP — понимание и генерация текста на естественных языках. Чат-бот должен понимать вводимый пользователем текст и генерировать на него должный ответ.\n",
        "Паттерны последовательностей слов, основанные на лингвистических признаках, могут быть полезны при реализации этих функций.\n",
        "\n",
        "С помощью паттернов последовательностей слов можно реагировать на\n",
        "высказывания пользователя и другим образом — например, генерируя\n",
        "подходящие утвердительные высказывания."
      ],
      "metadata": {
        "id": "X5jDGq_G9l2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "функция find_noun, принимающая два параметра.\n",
        "Первый из них содержит список предложений от начала текста до\n",
        "предложения, удовлетворяющего всем паттернам. В данном примере этот список будет включать все предложения текста, поскольку\n",
        "только последнее из них удовлетворяет всем паттернам. Однако\n",
        "существительное, обозначаемое местоимением, находится в одном из\n",
        "предыдущих предложений.\n",
        "Второй из передаваемых функции find_noun параметров — число местоимения, выступающего в роли прямого дополнения, которое удовлетворяет всем паттернам. Его определяет функция pron_pattern.\n",
        "Если значение этого аргумента 'plural', описываем список Python,\n",
        "содержащий теги уточненных частей речи, которые в spaCy отмечают\n",
        "существительные во множественном числе. Если же 'singular',\n",
        "создаем список тегов общих частей речи, обозначающих существительные в единственном числе."
      ],
      "metadata": {
        "id": "pq9D-gSrTkei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_noun(sents, num):#первый параметр содержит список предложений от начала текста до предложения, удовлет всем паттернам. \n",
        "#второй параметр - число местоимений, являющ прямым дополнением, удовлет всем паттернам\n",
        "  if num == 'plural':\n",
        "    taglist = ['NNS','NNPS']#для сущ во множ числе\n",
        "  if num == 'singular':\n",
        "    taglist = ['NN','NNP']#для сущ в единственном числе\n",
        "  for sent in reversed(sents):#проход в обратном порядке, с ближайшего предложения\n",
        "    for token in sent:#проход по токенам в каждом предложении\n",
        "      if token.tag_ in taglist:#происк токена, тег уточнения части речи\n",
        "        return token.text\n",
        "  return 'Noun not found'"
      ],
      "metadata": {
        "id": "ymNDIwAVTeli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проходим в цикле for по токенам предложения и ищем личное местоимение, которое выступает в роли прямого дополнения. Найдя его, генерируем новое высказывание. Меняем исходное предложение, заменяя личное местоимение соответствующим существительным и добавляя в конец too. После этого функция gen_utterance возвращает только что сгенерированное утвердительное высказывание.\n",
        "Если же найти прямое дополнение в форме личного местоимения не удалось, функция возвращает сообщение об ошибке "
      ],
      "metadata": {
        "id": "iXhDVeF1Tk7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_utterance(doc, noun):#функция генерации нового утвердительного высказывания\n",
        "  sent = ''\n",
        "  for i,token in enumerate(doc):#проход по токенам\n",
        "    if token.dep_ == 'dobj' and token.tag_ == 'PRP':#поиск личных местоимений, являющ дополнением\n",
        "      sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'#меняем личные местоимения на сущ\n",
        "      return sent#возвращение, что генерация прошла\n",
        "  return 'Failed to generate an utterance'#в случае если не было найдено"
      ],
      "metadata": {
        "id": "jl5dV9ufTeog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После применения конвейера к нашему образцу текста преобразуем текст в список предложений. Проходим в цикле по этому\n",
        "списку и ищем предложение, которое удовлетворяет описанным\n",
        "в функциях dep_pattern и pos_pattern паттернам. Затем с помощью\n",
        "функции find_noun определяем существительное, соответствующее\n",
        "местоимению из предложения, найденного на предыдущем шаге.\n",
        "Наконец, вызываем функцию gen_utterance для генерации соответствующего высказывания"
      ],
      "metadata": {
        "id": "WC6UGPnSTls4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#проверка работы\n",
        "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them promptly.')#заведем текст\n",
        "sents = list(doc.sents)#преобразование текста в список предложений\n",
        "response = ''\n",
        "noun = ''\n",
        "for i, sent in enumerate(sents):#проходим по циклу, ищем удовлетворяющих предложений\n",
        "  if dep_pattern(sent) and pos_pattern(sent):\n",
        "    noun = find_noun(sents[:i], pron_pattern(sent))#определяем сущ, соответствующее местоим из предложения\n",
        "    if noun != 'Noun not found':\n",
        "      response = gen_utterance(sents[i],noun)#вызываем функцию для генерации высказывания\n",
        "    break\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZAVFMXkTerP",
        "outputId": "e3125024-e641-4efc-c2d5-81c8b32adb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can recognize symbols too.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проход в цикле по главным элементам токенов\n",
        "\n",
        "Теперь разберемся, как выразить отношение между словами to и Berlin\n",
        "программным образом. Один из способов — обход дерева зависимостей\n",
        "слева направо, начиная с to, с проходом только по правосторонним\n",
        "дочерним элементам всех токенов. Если подобным образом можно\n",
        "пройти от to до Berlin, значит, можно обоснованно считать, что между\n",
        "этими двумя словами существует семантическая связь."
      ],
      "metadata": {
        "id": "CkdxS6kMCMnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В функции det_destination проходим в цикле по токенам полученного\n",
        "предложения в поисках сущности типа GPE. В случае ее обнаружения\n",
        "начинаем цикл while, в котором проходим по главным элементам всех\n",
        "токенов, начиная с токена, содержащего сущность типа GPE. Цикл завершается по достижении либо токена, содержащего to, либо корня предложения. Найти корень предложения можно путем сравнения каждого\n",
        "токена с его главным элементом, поскольку главный элемент корневого\n",
        "токена всегда ссылается на него. Или же можно проверять тег ROOT"
      ],
      "metadata": {
        "id": "9vZEocb_TmME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def det_destination(doc):#функция для определения места назначения\n",
        "  for i, token in enumerate(doc):\n",
        "    if token.ent_type != 0 and token.ent_type_ == 'GPE':#проход по токенам в поиске сущности типа GPE\n",
        "      while True:#в случае, если найдено\n",
        "        token = token.head#проход по главным элементам токенов\n",
        "        if token.text == 'to':\n",
        "          return doc[i].text#завершение текста до to\n",
        "        if token.head == token:#сравнение токена с главным элементом\n",
        "          return 'Failed to determine'\n",
        "  return 'Failed to determine'\n",
        "\n",
        "doc = nlp(u'I am going to the conference in Berlin.')\n",
        "dest = det_destination(doc)#\n",
        "print('It seems the user wants a ticket to ' + dest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mchIdpbvTeuH",
        "outputId": "c5bbd3f4-0d81-433e-c7d3-49156232a978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems the user wants a ticket to Berlin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Краткое изложение текста с помощью деревьев зависимостей\n",
        "\n",
        "Конечно, сфера применения подхода с деревьями синтаксических\n",
        "зависимостей не ограничивается чат-ботами. Этот подход можно\n",
        "\n",
        "использовать, например, в приложениях, предназначенных для обра-\n",
        "ботки отчетов. Представьте, что вам нужно разработать приложение\n",
        "\n",
        "для краткого изложения отчетов о розничных продажах, чтобы в них\n",
        "была лишь самая важная информация.\n",
        "\n",
        "Например, можно извлечь только предложения, содержащие числа. Та-\n",
        "ким образом получится краткий отчет об объемах продаж, доходах и за-\n",
        "тратах."
      ],
      "metadata": {
        "id": "lDOLv3MMDxAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проходим в цикле по токенам предложения в поисках токена, представляющего число. Когда таковой обнаруживаем, начинаем выполнение\n",
        "цикла while, в котором проходим по правосторонним главным элементам, стартуя с токена числа. Далее формируем новую фразу, присоединяя текст всех главных элементов в конец переменной phrase. С целью убедиться, что главный элемент следующего токена располагается\n",
        "справа от него, проверяем, включен ли токен в список левосторонних\n",
        "дочерних элементов его главного элемента."
      ],
      "metadata": {
        "id": "HQeBjgozTmuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"The product sales hit a new record in the first quarter, with 18.6 million units sold.\")\n",
        "phrase = ''\n",
        "for token in doc:\n",
        "  if token.pos_ == 'NUM':#поиск токена - число\n",
        "    while True:\n",
        "      #проходим по правосторонним главным элементам, начиная с токена числа\n",
        "      phrase = phrase + ' ' + token.text\n",
        "      token = token.head#формируем новую фразу, присоединяя в конец\n",
        "      if token not in list(token.head.lefts):#проверка включен ли токен в список левосторонних дочерних элементов его главного элемента\n",
        "        phrase = phrase + ' ' + token.text\n",
        "      if list(token.rights):\n",
        "        phrase = phrase + ' ' + doc[token.i+1:].text\n",
        "        break#прерывание, если условие ложно\n",
        "    break#прерывание внешнего\n",
        "print(phrase.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imItmyw7Tewu",
        "outputId": "f7d96675-1364-4ad0-ca71-2379b99dacaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.6 million units sold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обходим главные элементы токенов в цикле while, добавляя в конец\n",
        "формируемой фразы текст из этих элементов. По достижении смыслового глагола (с меткой ROOT)  прерываем выполнение цикла."
      ],
      "metadata": {
        "id": "muvClVeGTnOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  token = doc[token.i].head#обход главного элемента\n",
        "  if token.pos_ != 'ADP':\n",
        "    phrase = token.text + phrase#добавление в конец элемент\n",
        "  if token.dep_ == 'ROOT':#при достижении глагола \n",
        "    break#завершение"
      ],
      "metadata": {
        "id": "o2xQGbzeTey1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем с обхода в цикле дочерних элементов смыслового глагола в поисках подлежащего. Затем присоединяем к началу дочерние элементы подлежащего и подлежащее фразы, после чего выводим полученную фразу."
      ],
      "metadata": {
        "id": "aNFPhqGmTnyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tok in token.lefts:#обход в цикле дочернего элемента смыслового глагола\n",
        "  if tok.dep_ == 'nsubj':#поиск подлежащего\n",
        "    phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' ' + phrase\n",
        "    #присоединение к началу дочерние элементы подлежащего и подлежащее фразы\n",
        "    break\n",
        "print(phrase)#получаем фразу"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0QhuNaZTe1q",
        "outputId": "2f35a096-c08b-4f1a-c03f-2381e0c218ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The product sales hit 18.6 million units sold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Усовершенствование чат-бота для бронирования билетов с помощью учета контекста"
      ],
      "metadata": {
        "id": "MUF9FUWTGmPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код в функции guess_destination проходит в цикле по токенам предложения в поисках сущности типа GPE . После обнаружения функция\n",
        "возвращает ее вызывающему коду . Если же найти такую сущность\n",
        "не удалось, функция возвращает сообщение 'Failed to determine' ,\n",
        "означающее, что предложение не содержит сущности типа GPE."
      ],
      "metadata": {
        "id": "ad_QW7TTToOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#функция поиска в предложении GPE\n",
        "def guess_destination(doc):\n",
        "  for token in doc:\n",
        "    if token.ent_type != 0 and token.ent_type_ == 'GPE':#проход по циклу в поиске GPE\n",
        "      return token.text#в случае нахождения возвращает\n",
        "  return 'Failed to determine'#если не удалось найти"
      ],
      "metadata": {
        "id": "UmygEOreTe4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код функции gen_response начинается с вызова функции det_destination, которая определяет, содержит ли данное высказывание пару to + GPE. В случае обнаружения такой пары считаем, что пользователь хочет забронировать билет в указанный пункт назначения,\n",
        "поэтому уточняем время вылета.\n"
      ],
      "metadata": {
        "id": "uIMW72yPTozy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Генерация ответа\n",
        "def gen_response(doc):\n",
        "  dest = det_destination(doc)#вызов функции места назначения\n",
        "  if dest != 'Failed to determine':\n",
        "    return 'When do you need to be in ' + dest + '?'#уточняем время вылета в место назначения\n",
        "  dest = guess_destination(doc)#функция для поиска в предложении GPE\n",
        "  if dest != 'Failed to determine':\n",
        "    return 'You want a ticket to ' + dest +', right?'#уточнение действительно ли необходим вылет\n",
        "  return 'Are you flying somewhere?'#"
      ],
      "metadata": {
        "id": "8o5dzI5oTe6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для проверки кода в деле применяем к предложению конвейер, а затем\n",
        "передаем объект Doc в функцию gen_response, описанную в предыдущем листинге:\n"
      ],
      "metadata": {
        "id": "Io_yYKq0TpTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#проверка работы функции\n",
        "doc = nlp(u'I am going to the conference in Berlin.')\n",
        "print(gen_response(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1sp9ctl-cY8",
        "outputId": "448f81a1-8d5c-490b-8a64-493f8986daad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When do you need to be in Berlin?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Повышение IQ чат-бота за счет поиска подходящих модификаций\n",
        "\n",
        "Модификатор (modifier) — необязательный элемент фразы или простого предложения, меняющий смысл другого элемента. Удаление модификатора не всегда меняет основной смысл предложения, но делает его более общим."
      ],
      "metadata": {
        "id": "inpSdXqWIZ3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем с применения конвейера к короткому тексту, содержащему слово fruit с пре- и постмодификаторами. Объявляем два пустых списка: fruit_adjectives  и fruit_origins. В первом будут храниться все\n",
        "найденные для слова fruit прилагательные-модификаторы. Во втором —все найденные среди постмодификаторов слова fruit сущности типа GPE.\n",
        "\n",
        "Далее ищем в цикле по токенам всего текста слово fruit. Найдя его, сначала определяем прилагательные-премодификаторы этого слова,\n",
        "проходя по его левосторонним дочерним элементам и выбирая только прилагательные (определители и составные слова также могут быть\n",
        "премодификаторами). Добавляем прилагательные-модификаторы\n",
        "в список fruit_adjectives.\n",
        "\n",
        "Затем ищем постмодификаторы — в частности, именованные сущности, — проходя по правосторонним синтаксическим дочерним элементам слова fruit, и добавляем их в список fruit_origins.\n",
        "\n",
        "Сценарий выводит два списка:\n",
        "\n",
        "The list of adjectival modifiers for word fruit: ['nice', 'exotic']\n",
        "\n",
        "The list of GPE names applicable to word fruit as postmodifiers: ['Africa']\n",
        "\n",
        "Теперь наш бот «знает», что фрукт может быть приятным на вкус, экзотическим (или же и приятным, и экзотическим одновременно), а также может поставляться из Африки."
      ],
      "metadata": {
        "id": "HQFTiE3lTqBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"Kiwano has jelly-like flesh with a refreshingly fruity taste. This is a nice exotic fruit from Africa. It is definitely worth trying.\")\n",
        "fruit_adjectives = []#объявление списка, будут храниться все найденные для слова fruit прилагательные-модификаторы.\n",
        "fruit_origins = []#объявление списка все найденные среди постмодификаторов слова fruit сущности типа GPE.\n",
        "for token in doc:\n",
        "  if token.text == 'fruit':#поиск слова fruit по токенам\n",
        "  #определим прилагательные-премодификаторы слова, проходя по левосторонним элементам и выбирая прилагательные\n",
        "  #добавляем прилагательные в 1 список\n",
        "    fruit_adjectives = fruit_adjectives + [modifier.text for modifier\n",
        "    in token.lefts if modifier.pos_ == 'ADJ']\n",
        "  #ищем постмодификаторы - именованные сущности по правосторонним синтаксическим доч. элементам и добавляем во второй список\n",
        "    fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier\n",
        "    in token.rights if modifier.text == 'from' and\n",
        "doc[modifier.i + 1].ent_type != 0]\n",
        "#вывод вкуса\n",
        "print('The list of adjectival modifiers for word fruit:',\n",
        "  fruit_adjectives)\n",
        "#вывод места поставки\n",
        "print('The list of GPE names applicable to word fruit as postmodifiers:',\n",
        "  fruit_origins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsFHwCA-gcRE",
        "outputId": "b341f4d5-c8c3-446c-9088-7c978bc3f5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The list of adjectival modifiers for word fruit: ['nice', 'exotic']\n",
            "The list of GPE names applicable to word fruit as postmodifiers: ['Africa']\n"
          ]
        }
      ]
    }
  ]
}