{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrR6iFnknuKwEIYYJ3RJ0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LillySh/WishList/blob/main/NLP_HW_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IsvD8e8GI8EP"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens.doc import Doc\n",
        "from spacy.vocab import Vocab\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объект Doc можно создать и вручную"
      ],
      "metadata": {
        "id": "kKHy04tnJrIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "qcQlNLtVK_SQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Doc(Vocab(), words=[u'Hi', u'there'])\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOWMiFc9JVfL",
        "outputId": "d4fc4dd1-9ff0-4386-fe0e-8be2f1da4825"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hi there "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пройдемся по объекту Doc, чтобы разделить содержимое на токены"
      ],
      "metadata": {
        "id": "Qi1Puu3BJwBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GofB1BgnJa8P",
        "outputId": "d469bcc2-76fb-4543-e1a0-d2c7dc653ab1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Hi, there]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Например, нам нужно найти левосторонний дочерний элемент токена в дереве синтаксических зависимостей предложения. Такая операция позволяет найти прилагательное для заданного существительного.\n",
        "\n",
        "В doc указываем номер элемента (существительное), к которому ищем прилагательное."
      ],
      "metadata": {
        "id": "wUtWuia6LJZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I want a green apple.')\n",
        "[w for w in doc[4].lefts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g0vERVGJgVS",
        "outputId": "16564e20-5218-4e20-bd0a-21627bb45686"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a, green]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как у apple есть только левосторонние дочерние элементы, то doc[4].children будет давать аналогичный результат."
      ],
      "metadata": {
        "id": "mvucT1WiL-2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in doc[4].children]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHCT6uljLF6i",
        "outputId": "e876fa01-782c-4b6a-baab-356c918f64c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a, green]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью doc.sents текст можно разделить на отдельные предложения."
      ],
      "metadata": {
        "id": "aADo9eqfMLh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'A severe storm hit the beach. It started to rain.')\n",
        "for sent in doc.sents:\n",
        "  print([sent[i] for i in range(len(sent))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LJuMPrrL7V6",
        "outputId": "b960eb94-90a4-49ab-b16e-d71eaf2540b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A, severe, storm, hit, the, beach, .]\n",
            "[It, started, to, rain, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбираем предложения по индексу с помощью пере-\n",
        "числителя в цикле for: отфильтровав не интересующие нас предложения, проверяем только второе."
      ],
      "metadata": {
        "id": "BdCg_sg2MrGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,sent in enumerate(doc.sents):\n",
        "  if i==1 and sent[0].pos_== 'PRON':\n",
        "    print('The second sentence begins with a pronoun.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbtpOMiZMGVs",
        "outputId": "78aafffc-48b3-4663-d88b-ec626e0fafa8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second sentence begins with a pronoun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее выведем, сколько предложений в тексте оканчивается глаголом.\n",
        "\n",
        "Используем len(sent) - 2, так как: \n",
        "\n",
        "* во-первых, индексы всегда начинаются с 0 и заканчиваются на size-1, \n",
        "* во-вторых, последний токен в обоих предложениях точка, которую не нужно учитывать."
      ],
      "metadata": {
        "id": "weqgEQFtNCPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for sent in doc.sents:\n",
        "  if sent[len(sent)-2].pos_ == 'VERB':\n",
        "    counter+=1\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR322YelMurt",
        "outputId": "c22f1491-da61-4f2f-d1f3-6442d13f8d83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью свойства doc.noun_chunks объекта Doc можно пройти по именным фрагментам."
      ],
      "metadata": {
        "id": "95-LjyyeN37H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')\n",
        "for chunk in doc.noun_chunks:\n",
        "  print (chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edCXSHl8NOSj",
        "outputId": "569167c7-7f7a-4a3d-b488-1aed237e4512"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A noun chunk\n",
            "a phrase\n",
            "that\n",
            "a noun\n",
            "its head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для извлечения именных фрагментов можно обойти в цикле существительные в предложении и найти синтаксические дочерние элементы каждого существительного, чтобы из них образовать именные фрагменты. \n",
        "\n",
        "Сформируем именные фрагменты вручную:"
      ],
      "metadata": {
        "id": "jUNMZSQWOAZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc: #Проходим по токенам\n",
        "  if token.pos_=='NOUN':  #И выбираем только существительные\n",
        "    chunk = ''\n",
        "    for w in token.children: #Проходим по дочерним элементам существительных\n",
        "      if w.pos_ == 'DET' or w.pos_ == 'ADJ':  #Выбираем токены, которые определяют слова или прилагательное для именного фрагмента\n",
        "        chunk = chunk + w.text + ' '  #Присоединяем существительное к полученному фрагменту\n",
        "      chunk = chunk + token.text\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryUjCgEONyyX",
        "outputId": "f0ac484d-a542-4324-c243-ef3342d3e242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A chunkchunk\n",
            "a phrasephrase\n",
            "a nounnoun\n",
            "head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем Span для хранения множества смежных токенов документа"
      ],
      "metadata": {
        "id": "2awZR8VIQRyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp('I want a green apple.')\n",
        "doc[2:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO3OXopZO88J",
        "outputId": "cb7831d6-db4d-4b04-d5b2-144329ca5eb1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "a green apple"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предложение в следующем примере содержит два географических названия из нескольких слов, которые нам необходимо сгруппировать, — Golden Gate Bridge и San Francisco. При токенизации по умолчанию эти названия, состоящие из нескольких слов, не воспринимаются как единые токены."
      ],
      "metadata": {
        "id": "pknWYkaTmLxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n",
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "id": "IoMY_bEQQbdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0748de15-0211-4b05-bb48-c5abefa74b2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что каждому слову и знаку препинания соответствует отдельный токен.\n",
        "\n",
        "С помощью метода doc.retokenize() можно изменить поведение по умолчанию:"
      ],
      "metadata": {
        "id": "89_hoH0ymh2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with doc.retokenize() as retokenizer:\n",
        "  attrs = {\"LEMMA\": \"Golden Gate Bridge\"}\n",
        "  retokenizer.merge(doc[1:4], attrs=attrs)\n",
        "with doc.retokenize() as retokenizer:\n",
        "  attrs = {\"LEMMA\": \"San Francisco\"}\n",
        "  retokenizer.merge(doc[7:9], attrs=attrs)\n",
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fesWLiLJmfsT",
        "outputId": "245f94ee-bdd4-4586-e3ce-e3370f7e1ad4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The, Golden Gate Bridge, is, an, iconic, landmark, in, San Francisco, .]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, насколько корректно смогут обработать эту новую лемму лемматизатор и средства частеречной разметки и разбора зависимостей:"
      ],
      "metadata": {
        "id": "NvEtPgGXndRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.lemma_, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTUz9q9Nmps9",
        "outputId": "4b0463be-53b6-4ac3-92ed-82fdb80cdf35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The the DET det\n",
            "Golden Gate Bridge Golden Gate Bridge PROPN nsubj\n",
            "is be AUX ROOT\n",
            "an an DET det\n",
            "iconic iconic ADJ amod\n",
            "landmark landmark NOUN attr\n",
            "in in ADP prep\n",
            "San Francisco San Francisco PROPN pobj\n",
            ". . PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все приведенные в листинге атрибуты были корректно присвоены токену Golden Gate Bridge."
      ],
      "metadata": {
        "id": "XPPxUBBFnkIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Настроим конвейер обработки текста под свои нужды.**\n",
        "\n",
        "Посмотрим доступные для объекта nlp компоненты конвейера с помощью команды:"
      ],
      "metadata": {
        "id": "QpxC6Z9fnn7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yS3LwOkngZp",
        "outputId": "b12df76a-f811-4b69-ff0e-5f6625cfff58"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Далее отключим компоненты конвейера**\n",
        "\n",
        "Это можно сделать при создании объекта nlp, задав параметр disable:"
      ],
      "metadata": {
        "id": "91EuonJxn95f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser'])"
      ],
      "metadata": {
        "id": "uBJsimpinzjh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном случае мы создадим конвейер обработки без утилиты разбора зависимостей. При вызове такого экземпляра nlp для конкретного текста токены в этом тексте не получат метки зависимостей. \n"
      ],
      "metadata": {
        "id": "4wqmZPK8oVps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I want a green apple.')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX9f2Mm7oMPI",
        "outputId": "b21558a5-4655-4cc0-da4d-3f6ac8c8a538"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON \n",
            "want VERB \n",
            "a DET \n",
            "green ADJ \n",
            "apple NOUN \n",
            ". PUNCT \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы вывели для всех токенов предложения следующую\n",
        "информацию: текстовое содержимое, тег части речи и метку зависимостей. \n",
        "\n",
        "Однако метки зависимостей выведены не были."
      ],
      "metadata": {
        "id": "UTUjjObroiWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Настройка компонентов конвейера под свои нужды"
      ],
      "metadata": {
        "id": "_V09n0z80B-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "oyX3wQP3oeGm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка компонентов конвейера позволяет лучше решать задачи приложения. Допустим, система распознавания именованных сущностей вашей модели должна определять, что слово Festy означает один\n",
        "из районов города, а по умолчанию это слово считается названием организации, как показано в следующем примере:"
      ],
      "metadata": {
        "id": "f79p-vUr2ERy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQs4Wj8_0MJ7",
        "outputId": "951fb377-cb87-4e20-92b1-6cd9be8b83cb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метка ORG обозначает различные компании, государственные бюро и прочие учреждения. Но нам нужно, чтобы средство распознавания сущностей классифицировало его как сущность типа DISTRICT.\n",
        "\n",
        "В следующем примере сначала добавим новую метку DISTRICT в список поддерживаемых типов сущностей. Затем создадим обучающий пример данных и подадим его на вход средства распознавания сущностей, чтобы оно поняло, к чему должна относиться метка DISTRICT."
      ],
      "metadata": {
        "id": "Dq_3cN4V2J3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL = 'DISTRICT'\n",
        "TRAIN_DATA = [('We need to deliver it to Festy.', {'entities': [(25, 30, 'DISTRICT')]}),('I like red oranges', {\n",
        "'entities': []\n",
        "})\n",
        "]"
      ],
      "metadata": {
        "id": "XVCEjzTp139z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для простоты обучающий набор состоит лишь из двух примеров данных. Каждый из обучающих примеров включает в себя предложение, содержащее или не содержащее интересующую нас сущность (сущности), которой должна присваиваться эта новая метка сущности. Если в примере данных имеется  нужная сущность, указываем ее начальную и конечную позиции. \n",
        "Второе предложение в обучающем наборе данных вообще не содержит слова Festy."
      ],
      "metadata": {
        "id": "YkN2BIk82bPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий этап — добавление новой метки сущности DISTRICT в компонент распознавания сущностей. Но сначала необходимо получить экземпляр компонента конвейера ner:"
      ],
      "metadata": {
        "id": "dOHYMzxD2oUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "id": "wx9FGlZp2Xnp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выполнив этот шаг, в полученный объект ner можно добавить новую\n",
        "# метку с помощью метода ner.add_label():\n",
        "ner.add_label(LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaUR8Pyo2qxo",
        "outputId": "4c20ccdd-1363-4d17-b6ac-e65824001f84"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отключим остальные конвейеры, чтобы во время обучения \n",
        "# обновлялся только компонент распознавания сущностей:\n",
        "nlp.disable_pipes('tagger')\n",
        "nlp.disable_pipes('parser')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqeODZ-z2uq8",
        "outputId": "e50a2ad4-70cd-4b8d-8909-c1b55bfc8062"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parser']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь можно начинать обучение компонента распознаванию сущностей на примерах данных из списка TRAIN_DATA, который был создан ранее:"
      ],
      "metadata": {
        "id": "tHCvS0Rg3E7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.create_optimizer()\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "for i in range(25):\n",
        "  random.shuffle(TRAIN_DATA)\n",
        "  for text, annotations in TRAIN_DATA:\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    nlp.update([example], sgd=optimizer)"
      ],
      "metadata": {
        "id": "kXuzUcn726YC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По завершении выполнения можно проверить, как обновленный оптимизатор распознает токен Festy:"
      ],
      "metadata": {
        "id": "I1Bx0-Bu3bN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5styY9Uz3LJl",
        "outputId": "59d0bfde-c7fb-4da0-cf13-e7d7ad37c7aa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy ORG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим из выведенных результатов, всё работает отлично."
      ],
      "metadata": {
        "id": "RG0AzeRw3inc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Использование структур данных уровня языка С библиотеки spaCy\n",
        "\n",
        "Установим Cython с помощью pip:"
      ],
      "metadata": {
        "id": "IM1Bv6fH5I7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZLVzkpj4U6e",
        "outputId": "8bde1a25-7b56-4da7-c15e-0e1f04f9fafe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Сценарий Cython**\n",
        "\n",
        "Создаем в одном из каталогов локальной файловой системы файл spacytext.pyx и вставляем в него следующий код:\n",
        "\n",
        "Код на языке Cython, в отличие от написанного на Python, необходимо компилировать."
      ],
      "metadata": {
        "id": "Aegw0FJ65t9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('spacytext.pyx', 'w')"
      ],
      "metadata": {
        "id": "mTm8-nUy3fbk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.write(\"\"\"from cymem.cymem cimport Pool\n",
        "from spacy.tokens.doc cimport Doc\n",
        "from spacy.structs cimport TokenC\n",
        "from spacy.typedefs cimport hash_t\n",
        "\n",
        "cdef struct DocStruct:\n",
        "  TokenC* c\n",
        "  int length\n",
        "\n",
        "cdef int counter(DocStruct* doc, hash_t tag):\n",
        "  cdef int cnt = 0\n",
        "  for c in doc.c[:doc.length]:\n",
        "    if c.tag == tag:\n",
        "      cnt += 1\n",
        "  return cnt\n",
        "\n",
        "cpdef main(Doc mydoc):\n",
        "  cdef int cnt\n",
        "  cdef Pool mem = Pool()\n",
        "  cdef DocStruct* doc_ptr = <DocStruct*>mem.alloc(1, sizeof(DocStruct))\n",
        "  doc_ptr.c = mydoc.c\n",
        "  doc_ptr.length = mydoc.length\n",
        "  tag = mydoc.vocab.strings.add('PRP')\n",
        "  cnt = counter(doc_ptr, tag)\n",
        "  print(doc_ptr.length)\n",
        "  print(cnt)\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoekiyzX4EGY",
        "outputId": "2340953a-b73c-46f0-d94f-5989879cf4a3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "623"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.close()"
      ],
      "metadata": {
        "id": "acda2KcF5OHe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сборка модуля Cython\n",
        "Создаем файл setup.py в каталоге, где располагается наш сценарий Cython. Файл должен содержать следующий код:"
      ],
      "metadata": {
        "id": "xo3WzM8y6St8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('setup.py', 'w')"
      ],
      "metadata": {
        "id": "w-QzbTRV6AVN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.write('''from distutils.core import setup\n",
        "from Cython.Build import cythonize\n",
        "\n",
        "import numpy\n",
        "setup(name='spacy text app',\n",
        "      ext_modules=cythonize(\"spacytext.pyx\", language=\"c++\"),\n",
        "      include_dirs=[numpy.get_include()]\n",
        "      )''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDjGxXtp6f39",
        "outputId": "86f5349f-dbb9-49e5-bc24-57ce3a131ece"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.close()"
      ],
      "metadata": {
        "id": "50fYJXQO6hj1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подготовки установочного сценария компилируем код Cython. Сделать это можно из системного терминала:"
      ],
      "metadata": {
        "id": "oCnzcc3m6kzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmBIIAKR6ipu",
        "outputId": "7caf6add-334a-41f6-c490-f6abedde2c7b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: passing language='c++' to cythonize() is deprecated. Instead, put \"# distutils: language=c++\" in your .pyx or .pxd file(s)\n",
            "Compiling spacytext.pyx because it changed.\n",
            "[1/1] Cythonizing spacytext.pyx\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/spacytext.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "running build_ext\n",
            "building 'spacytext' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c spacytext.cpp -o build/temp.linux-x86_64-3.7/spacytext.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kspacytext.cpp:774\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/spacytext.o -o /content/spacytext.cpython-37m-x86_64-linux-gnu.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Тестирование модуля**\n",
        "\n",
        "После успешного завершения процесса компиляции модуль spacytext будет добавлен в среду Python. Для его тестирования откройте сеанс Python и выполните команду:"
      ],
      "metadata": {
        "id": "h8pR7Rqu6or6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacytext import main"
      ],
      "metadata": {
        "id": "IsFRtBPu6m0n"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "f= open(\"test.txt\",\"rb\")\n",
        "contents =f.read()\n",
        "doc = nlp(contents[:100000].decode('utf8'))\n",
        "main(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJOHIIFF6um9",
        "outputId": "e6d4a2fc-d3e7-40f9-83f4-3b565ea957fc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2685\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первое число означает общее количество токенов, найденных в тексте. \n",
        "\n",
        "Второе число представляет собой количество обнаруженных личных местоимений."
      ],
      "metadata": {
        "id": "qy7q3gGy-g14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выделение и использование лингвистических признаков.\n",
        "\n",
        "### **Теги для чисел, символов и знаков препинания.**\n",
        "\n",
        "Для начала выделим из токенов признаки общих частей речи и увидим, как spaCy распознает различные части речи:\n",
        "\n",
        "Мы создали для входного предложения объект Doc и вывели теги общих частей речи, а также воспользовались функцией spacy.explain(), которая возвращает описание для заданного лингвистического признака."
      ],
      "metadata": {
        "id": "0uudN6zV-puq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
        "for token in doc:\n",
        "  print(\"{:10}\\t{:10}\\t{}\".format(token.text, token.pos_, spacy.explain(token.pos_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXd5D8T6wZw",
        "outputId": "eed82f78-7255-449f-e84a-6652e4f53034"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The       \tDET       \tdeterminer\n",
            "firm      \tNOUN      \tnoun\n",
            "earned    \tVERB      \tverb\n",
            "$         \tSYM       \tsymbol\n",
            "1.5       \tNUM       \tnumeral\n",
            "million   \tNUM       \tnumeral\n",
            "in        \tADP       \tadposition\n",
            "2017      \tNUM       \tnumeral\n",
            ".         \tPUNCT     \tpunctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, было распознано даже числительное «миллион» в буквенном виде."
      ],
      "metadata": {
        "id": "x3VAvJmLAR6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь сравним теги общих и уточненных частей речи для того же предложения, выведя в отдельном столбце описание для тегов уточненных частей речи:"
      ],
      "metadata": {
        "id": "gHH6V-oEAXe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE4oFRJ7AIJr",
        "outputId": "d01232da-671d-4ea4-d257-21f4b2164a0d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET DT determiner\n",
            "firm NOUN NN noun, singular or mass\n",
            "earned VERB VBD verb, past tense\n",
            "$ SYM $ symbol, currency\n",
            "1.5 NUM CD cardinal number\n",
            "million NUM CD cardinal number\n",
            "in ADP IN conjunction, subordinating or preposition\n",
            "2017 NUM CD cardinal number\n",
            ". PUNCT . punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Второй и третий столбцы содержат теги общих и уточненных частей речи соответственно. В четвертом столбце приведено описание тегов уточненных частей речи из третьего столбца."
      ],
      "metadata": {
        "id": "y4_O2NymAfIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Выделение описаний денежных сумм**"
      ],
      "metadata": {
        "id": "1EE3owzCAqs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий сценарий иллюстрирует, как фразу \"$1.5 million\" можно выделить из предложения на основе одних лишь тегов частей речи токенов. Можете сохранить этот сценарий в файле и затем выполнить код из сеанса Python:\n",
        "\n",
        "В этом коде проходим по токенам предложения в цикле в поисках токена с тегом $ уточненной части речи. Данный тег обозначает символ валюты, с которого обычно начинается фраза, описывающая некоторую сумму денег."
      ],
      "metadata": {
        "id": "JZZz2Ff1AyU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
        "phrase = ''\n",
        "for token in doc:\n",
        "  if token.tag_ == '$':\n",
        "    phrase = token.text\n",
        "    i = token.i+1\n",
        "    while doc[i].tag_ == 'CD':\n",
        "      phrase += doc[i].text + ' '\n",
        "      i += 1\n",
        "    break\n",
        "phrase = phrase[:-1]\n",
        "print(phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWQctEr7AcHG",
        "outputId": "994e7f39-20bb-42f9-d703-3736071467f7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$1.5 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Преобразование утвердительных высказываний в вопросительные\n",
        "\n",
        "Предложение содержит несколько глаголов и местоимений, причем с различной морфологией. Чтобы в этом убедиться, взглянем на теги частей речи, которые spaCy присваивает токенам предложения:"
      ],
      "metadata": {
        "id": "j7qfsn8mA9cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  print(\"{:7}\\t{:5}\\t{}\".format(token.text, token.pos_, token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FOg6Sd7A203",
        "outputId": "74fc6bd8-c221-425b-a477-cbdb5efe49f8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I      \tPRON \tPRP\n",
            "can    \tAUX  \tMD\n",
            "promise\tVERB \tVB\n",
            "it     \tPRON \tPRP\n",
            "is     \tAUX  \tVBZ\n",
            "worth  \tADJ  \tJJ\n",
            "your   \tPRON \tPRP$\n",
            "time   \tNOUN \tNN\n",
            ".      \tPUNCT\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные шаги генерации вопроса из исходного утверждения.\n",
        "\n",
        "1. Поменять порядок слов в исходном \n",
        "предложении с «подлежащее + вспомогательный модальный глагол + глагол в неопределенной форме» на «модальный вспомогательный глагол + глагол в неопределенной форме + подлежащее».\n",
        "2. Заменить личное местоимение I (подлежащее в предложении) на you.\n",
        "3. Заменить притяжательное местоимение your на my.\n",
        "4. Вставить наречие-модификатор really перед словом promise для усиления последнего.\n",
        "5. Заменить знак препинания . на ? в конце предложения.\n",
        "\n",
        "\n",
        "Эти шаги реализованы в следующем сценарии:"
      ],
      "metadata": {
        "id": "zDFRB-MSBOrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2g6-WMtBFC4",
        "outputId": "5ec313b9-d68a-47e9-fd1f-01a9a05efb4a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I can promise it is worth your time."
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде всего проходим в цикле по токенам предложения и меняем местами существительное и глагол, чтобы предложение стало вопросительным."
      ],
      "metadata": {
        "id": "hZqty_8bBeb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "sent = ''\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':\n",
        "    sent = doc[i+1].text.capitalize() + ' ' + doc[i].text\n",
        "    sent = sent + ' ' + doc[i+2:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HcNQGH9mBd-A",
        "outputId": "77640d1d-578f-4313-9c65-0d4fc75d09eb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can I promise it is worth your time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее создаем новый цикл for, который заменит личное местоимение I личным местоимением you. Для этого ищем личные местоимения (помеченные тегами PRP). Если личное местоимение — I, меняем его на you и выходим из цикла for."
      ],
      "metadata": {
        "id": "MdV170HKBm-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP' and token.text == 'I':\n",
        "    print(doc[:i].text)\n",
        "    sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W4hOyHvrBkNm",
        "outputId": "1eeeb8c8-9159-408d-cf49-1fd3d8ab1395"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you promise it is worth your time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Повторяем этот процесс. Ищем тег PRP$ и меняем притяжательное местоимение your на my."
      ],
      "metadata": {
        "id": "j1gWVNkzBrst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP$' and token.text == 'your':\n",
        "    sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gQvFIOw5Booy",
        "outputId": "a0ecd0a4-d439-4a24-ac88-854c981d258c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you promise it is worth my time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В новом цикле for находим глагол в неопределенной форме и вставляем перед ним наречие-модификатор really"
      ],
      "metadata": {
        "id": "R-FR56QABwQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'VB':\n",
        "    sent = doc[:i].text + ' really ' + doc[i:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a7vsUX8dBtC5",
        "outputId": "f9d1cfc0-3d78-4f85-f7d2-83b9979495ea"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you really promise it is worth my time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, заменяем точку в конце предложения вопросительным знаком: это единственный шаг, для которого не нужен цикл."
      ],
      "metadata": {
        "id": "_IdpYWRGB0hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "sent = doc[:len(doc)-1].text + '?'\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aToI-QUAByB7",
        "outputId": "a1ff194c-43c1-4736-cf59-e795d4632607"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you really promise it is worth my time?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Использование меток синтаксических зависимостей при обработке языка\n",
        "### Различаем подлежащие и дополнения\n",
        "Чтобы определить программным образом, чем в заданном предложении являются такие местоимения, как you или it, необходимо посмотреть на присвоенную им метку зависимости. Теги частей речи в сочетании с метками зависимостей позволяют получить гораздо больше информации о роли токена в предложении.\n",
        "\n",
        "Второй и третий столбцы содержат теги общих и уточненных частей речи соответственно. Четвертый столбец содержит метки зависимостей, а пятый — описания этих меток."
      ],
      "metadata": {
        "id": "O1o9MJ7dB8Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.tag_, token.dep_, spacy.\n",
        "explain(token.dep_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1x_qyuUB18A",
        "outputId": "baade6c4-a78e-423d-8052-59e6e015067b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON PRP nsubj nominal subject\n",
            "can AUX MD aux auxiliary\n",
            "promise VERB VB ROOT root\n",
            "it PRON PRP nsubj nominal subject\n",
            "is AUX VBZ ccomp clausal complement\n",
            "worth ADJ JJ acomp adjectival complement\n",
            "your PRON PRP$ poss possession modifier\n",
            "time NOUN NN npadvmod noun phrase as adverbial modifier\n",
            ". PUNCT . punct punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для токенов предложения были извлечены теги частей речи, метки зависимостей и их описание."
      ],
      "metadata": {
        "id": "bCdaPCJoCV_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Выясняем, какой вопрос должен задать чат-бот**\n",
        "Начнем с импорта модуля sys, который позволяет получить предложение в виде аргумента для дальнейшей обработки:"
      ],
      "metadata": {
        "id": "0Z_XhtrHDtmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import sys\n",
        "from spacy.tokens.doc import Doc\n",
        "from spacy.vocab import Vocab"
      ],
      "metadata": {
        "id": "_rAbSVXrCT-n"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее опишем функцию для распознавания и извлечения произвольного именного фрагмента — прямого дополнения из входного документа. Например, если вы ввели документ, содержащий предложение I want a green apple., то будет возвращен фрагмент a green apple:"
      ],
      "metadata": {
        "id": "1jdezFKSD41c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chunk(doc):\n",
        "  chunk = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj':\n",
        "      shift = len([w for w in token.children])\n",
        "      #print([w for w in token.children])\n",
        "      сhunk = doc[i-shift:i+1]\n",
        "      break\n",
        "  return chunk"
      ],
      "metadata": {
        "id": "-O-D04CCD4V_"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция просматривает фрагмент и определяет, какой тип вопроса должен задать чат-бот:\n",
        "\n",
        "Сначала задаем начальное значение переменной question_type равным 'yes/no', что соответствует вопросу типа «да/нет». Далее в переданном в функцию chunk ищем токен с тегом 'amod' который означает прилагательное-модификатор. Если таковое находится, меняем значение переменной question_type на 'info', соответствующее информационному типу вопроса."
      ],
      "metadata": {
        "id": "U-sHRh7vEAd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_question_type(chunk):\n",
        "  question_type = 'yesno'\n",
        "  for token in chunk:\n",
        "    if token.dep_ == 'amod':\n",
        "      question_type = 'info'\n",
        "  return question_type"
      ],
      "metadata": {
        "id": "a7WG937ID-4E"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определив, какой тип вопроса нам нужен, генерируем в следующей функции вопрос на основе входного предложения:"
      ],
      "metadata": {
        "id": "THf2BGrdEH66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(doc, question_type):\n",
        "  sent = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
        "      sent = 'do ' + doc[i].text\n",
        "      sent = sent + ' ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and token.text == 'I':\n",
        "      sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  if question_type == 'info':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
        "        break\n",
        "  if question_type == 'yesno':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
        "        break\n",
        "  doc=nlp(sent)\n",
        "  sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'\n",
        "  return sent"
      ],
      "metadata": {
        "id": "Wnieo74QEGTu"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание: используемый алгоритм предполагает, что входное предложение оканчивается знаком препинания, например . или !.\n",
        "\n",
        "После описания всех функций посмотрим на основной блок сценария:"
      ],
      "metadata": {
        "id": "97cBS1TZEMTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chunk(doc):\n",
        "  chunk = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj':\n",
        "      shift = len([w for w in token.children])\n",
        "      chunk = doc[i-shift:i+1]\n",
        "      break\n",
        "  # print(chunk)\n",
        "  return chunk\n",
        "\n",
        "def determine_question_type(chunk):\n",
        "  question_type = 'yesno'\n",
        "  for token in chunk:\n",
        "    if token.dep_ == 'amod':\n",
        "      question_type = 'info'\n",
        "  return question_type\n",
        "\n",
        "def generate_question(doc, question_type):\n",
        "  sent = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
        "      sent = 'do ' + doc[i].text\n",
        "      sent = sent + ' ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and token.text == 'I':\n",
        "      sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  if question_type == 'info':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
        "        break\n",
        "  if question_type == 'yesno':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
        "        break\n",
        "  doc=nlp(sent)\n",
        "  sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'\n",
        "  return sent\n",
        "def chat_bot(new_str):\n",
        "  if len(new_str) > 1:\n",
        "    sent = new_str\n",
        "    # print(\"Sent: \" + sent)\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(sent)\n",
        "    # print(f\"Doc: {doc}\")\n",
        "    chunk = find_chunk(doc)\n",
        "    # print(f\"Chunk: {chunk}\")\n",
        "    if chunk == None:\n",
        "      print('The sentence does not contain a direct object.')\n",
        "      sys.exit()\n",
        "    question_type = determine_question_type(chunk)\n",
        "    question = generate_question(doc, question_type)\n",
        "    print(question)\n",
        "  else:\n",
        "    print('You did not submit a sentence!')\n",
        "\n",
        "\n",
        "s1 = 'I want a green apple.'\n",
        "print(s1)\n",
        "chat_bot(s1)\n",
        "s2 = 'I want an apple.'\n",
        "print(s2)\n",
        "chat_bot(s2)\n",
        "s3 = 'I want...'\n",
        "print(s3)\n",
        "chat_bot(s3)\n",
        "s4 = \"\"\n",
        "print(s4)\n",
        "chat_bot(s4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT17f4h9EJ1y",
        "outputId": "877d731f-c5ca-4b1d-88c9-8471e66471da"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want a green apple.\n",
            "Why do you want a green one?\n",
            "I want an apple.\n",
            "Do you want a red apple?\n",
            "I want...\n",
            "Do you want?\n",
            "\n",
            "You did not submit a sentence!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b39AY3SNEbRt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}